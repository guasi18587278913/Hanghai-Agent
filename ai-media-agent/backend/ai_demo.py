#!/usr/bin/env python3
"""
AIå¢å¼ºç‰ˆDemo - é›†æˆGemini-2.5-pro
ä½¿ç”¨OpenRouter API
"""
from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import random
import urllib.request
import urllib.parse
from pathlib import Path

# OpenRouteré…ç½®
OPENROUTER_API_KEY = "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"
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
MODEL = "google/gemini-flash-1.5"  # ä½¿ç”¨å¯ç”¨çš„Geminiæ¨¡å‹

# åŠ è½½æ•°æ®
def load_data():
    """åŠ è½½æ¨¡æ‹Ÿæ•°æ®"""
    data = {}
    
    # Q&Aæ•°æ®
    try:
        with open('../data/qa.json', 'r', encoding='utf-8') as f:
            data['qa'] = json.load(f)['qa_pairs']
    except:
        data['qa'] = []
    
    # å­¦ä¹ è·¯å¾„
    try:
        with open('../data/learning_path.json', 'r', encoding='utf-8') as f:
            data['learning_path'] = json.load(f)
    except:
        data['learning_path'] = {"stages": []}
    
    # çˆ†æ¬¾æ¡ˆä¾‹ï¼ˆç®€åŒ–å¤„ç†CSVï¼‰
    try:
        cases = []
        with open('../data/cases.csv', 'r', encoding='utf-8') as f:
            lines = f.readlines()
            headers = lines[0].strip().split(',')
            for line in lines[1:]:
                values = line.strip().split(',')
                case = dict(zip(headers, values))
                cases.append(case)
        data['cases'] = cases
    except:
        data['cases'] = []
    
    return data

# å…¨å±€æ•°æ®
DATA = load_data()

def call_gemini(prompt):
    """è°ƒç”¨Gemini API"""
    try:
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIè‡ªåª’ä½“å­¦ä¹ åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·å­¦ä¹ AIè‡ªåª’ä½“ç›¸å…³çŸ¥è¯†ã€‚
ä½ æœ‰ä»¥ä¸‹çŸ¥è¯†åº“ï¼š
1. AIè‡ªåª’ä½“åŸºç¡€æ¦‚å¿µå’Œå®šä¹‰
2. 21å¤©å­¦ä¹ è®¡åˆ’ï¼ˆåˆ†ä¸ºå®šä½ã€æ­å»ºã€è¿è¥ä¸‰ä¸ªé˜¶æ®µï¼‰
3. çˆ†æ¬¾æ¡ˆä¾‹åˆ†ææ•°æ®
4. å¹³å°è¿è¥æŠ€å·§ï¼ˆå°çº¢ä¹¦ã€æŠ–éŸ³ç­‰ï¼‰
5. å˜ç°æ–¹å¼å’Œå•†ä¸šåŒ–ç­–ç•¥

è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œæä¾›ä¸“ä¸šã€å®ç”¨ã€å¯æ“ä½œçš„å»ºè®®ã€‚å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºã€‚"""

        # å‡†å¤‡è¯·æ±‚æ•°æ®
        request_data = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 500
        }
        
        # åˆ›å»ºè¯·æ±‚
        req = urllib.request.Request(
            OPENROUTER_API_URL,
            data=json.dumps(request_data).encode('utf-8'),
            headers={
                'Authorization': f'Bearer {OPENROUTER_API_KEY}',
                'Content-Type': 'application/json',
                'HTTP-Referer': 'http://localhost:8000',
                'X-Title': 'AI Media Agent Demo'
            }
        )
        
        # å‘é€è¯·æ±‚
        with urllib.request.urlopen(req, timeout=15) as response:
            result = json.loads(response.read().decode('utf-8'))
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                print(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                return None
    
    except urllib.error.HTTPError as e:
        error_body = e.read().decode('utf-8')
        print(f"API HTTPé”™è¯¯ {e.code}: {error_body}")
        return None
    except Exception as e:
        print(f"APIè°ƒç”¨é”™è¯¯: {e}")
        return None

class AIHandler(BaseHTTPRequestHandler):
    """AIå¢å¼ºçš„HTTPå¤„ç†å™¨"""
    
    def do_OPTIONS(self):
        """å¤„ç†CORSé¢„æ£€è¯·æ±‚"""
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()
    
    def do_GET(self):
        """å¤„ç†GETè¯·æ±‚"""
        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.send_header('Access-Control-Allow-Origin', '*')
        self.end_headers()
        
        if self.path == '/':
            response = {
                "message": "AIè‡ªåª’ä½“å­¦ä¹ Agent - AIå¢å¼ºç‰ˆ",
                "model": "Gemini-2.0-flash via OpenRouter",
                "endpoints": {
                    "/chat": "POST - AIæ™ºèƒ½é—®ç­”",
                    "/progress": "GET - å­¦ä¹ è¿›åº¦",
                    "/cases": "GET - çˆ†æ¬¾æ¡ˆä¾‹",
                    "/path": "GET - å­¦ä¹ è·¯å¾„",
                    "/analyze": "POST - AIå†…å®¹åˆ†æ"
                }
            }
        elif self.path == '/progress':
            # æ¨¡æ‹Ÿè¿›åº¦
            day = random.randint(1, 21)
            stage_idx = 0 if day <= 4 else (1 if day <= 11 else 2)
            stage = DATA['learning_path']['stages'][stage_idx]
            
            # æ‰¾åˆ°ä»Šå¤©çš„ä»»åŠ¡
            today_task = None
            for task in stage['tasks']:
                if task['day'] == day:
                    today_task = task
                    break
            
            response = {
                "current_day": day,
                "total_days": 21,
                "progress": int(day/21*100),
                "stage": stage['name'],
                "stage_description": stage['description'],
                "today_task": today_task
            }
        elif self.path == '/cases':
            response = {
                "cases": DATA['cases'][:5],
                "total": len(DATA['cases'])
            }
        elif self.path == '/path':
            response = DATA['learning_path']
        else:
            response = {"error": "Not found"}
        
        self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
    
    def do_POST(self):
        """å¤„ç†POSTè¯·æ±‚"""
        # è¯»å–è¯·æ±‚ä½“
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)
        
        try:
            data = json.loads(post_data.decode('utf-8'))
            
            if self.path == '/chat':
                response = self._process_chat(data.get('message', ''))
            elif self.path == '/analyze':
                response = self._analyze_content(data.get('content', ''), data.get('platform', 'å°çº¢ä¹¦'))
            elif self.path == '/generate-path':
                response = self._generate_personalized_path(data.get('profile', {}))
            else:
                self.send_error(404)
                return
            
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
        except Exception as e:
            self.send_error(500, str(e))
    
    def _process_chat(self, message):
        """å¤„ç†èŠå¤©æ¶ˆæ¯ - ä½¿ç”¨AI"""
        message_lower = message.lower()
        
        # å…ˆå°è¯•ä»çŸ¥è¯†åº“ç²¾ç¡®åŒ¹é…
        for qa in DATA['qa']:
            if any(keyword in message_lower for keyword in qa['question'].lower().split()):
                # ä½¿ç”¨AIå¢å¼ºå›ç­”
                enhanced_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{message}
                
å‚è€ƒç­”æ¡ˆï¼š{qa['answer']}

è¯·åŸºäºå‚è€ƒç­”æ¡ˆï¼Œç»“åˆä½ çš„çŸ¥è¯†ï¼Œç»™å‡ºæ›´è¯¦ç»†å’Œå®ç”¨çš„å›ç­”ã€‚ä¿æŒä¸“ä¸šæ€§ï¼Œå¢åŠ å…·ä½“çš„æ“ä½œå»ºè®®ã€‚"""
                
                ai_answer = call_gemini(enhanced_prompt)
                if ai_answer:
                    return {
                        "answer": ai_answer,
                        "type": "ai_enhanced",
                        "source": f"AIå¢å¼º - {qa['category']}",
                        "confidence": 0.95
                    }
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨AIå›ç­”
        # æ„å»ºåŒ…å«ä¸Šä¸‹æ–‡çš„æç¤ºè¯
        context_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼š{message}

è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”ï¼š
1. æˆ‘ä»¬æœ‰21å¤©çš„AIè‡ªåª’ä½“å­¦ä¹ è®¡åˆ’ï¼Œåˆ†ä¸ºå®šä½ï¼ˆ1-4å¤©ï¼‰ã€æ­å»ºï¼ˆ5-11å¤©ï¼‰ã€è¿è¥ï¼ˆ12-21å¤©ï¼‰ä¸‰ä¸ªé˜¶æ®µ
2. ä¸»è¦å¹³å°æ˜¯å°çº¢ä¹¦å’ŒæŠ–éŸ³
3. å˜ç°æ–¹å¼åŒ…æ‹¬å¹¿å‘Šåˆä½œã€çŸ¥è¯†ä»˜è´¹ã€ä¸“ä¸šæœåŠ¡ã€æµé‡åˆ†æˆ
4. æˆ‘ä»¬æœ‰çˆ†æ¬¾æ¡ˆä¾‹æ•°æ®å¯ä»¥å‚è€ƒ

è¯·ç»™å‡ºä¸“ä¸šã€å…·ä½“ã€å¯æ“ä½œçš„å»ºè®®ã€‚"""

        ai_answer = call_gemini(context_prompt)
        
        if ai_answer:
            return {
                "answer": ai_answer,
                "type": "ai_generated",
                "source": "Gemini AI",
                "suggestions": self._generate_suggestions(message)
            }
        else:
            # é™çº§åˆ°è§„åˆ™åŒ¹é…
            return self._fallback_response(message)
    
    def _analyze_content(self, content, platform):
        """AIåˆ†æå†…å®¹"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹{platform}å¹³å°çš„å†…å®¹æ ‡é¢˜ï¼Œå¹¶ç»™å‡ºä¼˜åŒ–å»ºè®®ï¼š

æ ‡é¢˜ï¼š{content}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æï¼š
1. æ ‡é¢˜å¸å¼•åŠ›ï¼ˆ1-10åˆ†ï¼‰
2. å…³é”®è¯å¸ƒå±€
3. æƒ…ç»ªä»·å€¼
4. ç›®æ ‡å—ä¼—åŒ¹é…åº¦
5. çˆ†æ¬¾æ½œåŠ›é¢„æµ‹

ç»™å‡ºå…·ä½“çš„ä¼˜åŒ–å»ºè®®å’Œæ”¹è¿›æ–¹å‘ã€‚"""

        ai_analysis = call_gemini(prompt)
        
        if ai_analysis:
            return {
                "analysis": ai_analysis,
                "platform": platform,
                "original_content": content,
                "type": "ai_analysis"
            }
        else:
            # é™çº§å“åº”
            return {
                "analysis": "æ ‡é¢˜è¿˜ä¸é”™ï¼Œå»ºè®®åŠ å…¥æ•°å­—å’Œç»“æœå¯¼å‘çš„è¯æ±‡",
                "score": random.randint(60, 85),
                "type": "rule_based"
            }
    
    def _generate_suggestions(self, message):
        """ç”Ÿæˆç›¸å…³å»ºè®®"""
        suggestions = []
        
        if "æ–°æ‰‹" in message or "å¼€å§‹" in message:
            suggestions = ["äº†è§£21å¤©å­¦ä¹ è®¡åˆ’", "æŸ¥çœ‹çˆ†æ¬¾æ¡ˆä¾‹", "é€‰æ‹©é€‚åˆçš„å¹³å°"]
        elif "å˜ç°" in message or "èµšé’±" in message:
            suggestions = ["äº†è§£å˜ç°é—¨æ§›", "æŸ¥çœ‹æˆåŠŸæ¡ˆä¾‹", "åˆ¶å®šå†…å®¹ç­–ç•¥"]
        elif "å†…å®¹" in message or "åˆ›ä½œ" in message:
            suggestions = ["åˆ†æçˆ†æ¬¾æ ‡é¢˜", "å­¦ä¹ å†…å®¹æ¨¡æ¿", "äº†è§£å¹³å°è§„åˆ™"]
        else:
            suggestions = ["æŸ¥çœ‹å­¦ä¹ è¿›åº¦", "æµè§ˆçˆ†æ¬¾æ¡ˆä¾‹", "è·å–ä»Šæ—¥ä»»åŠ¡"]
        
        return suggestions[:3]
    
    def _fallback_response(self, message):
        """é™çº§å“åº”ï¼ˆå½“AIä¸å¯ç”¨æ—¶ï¼‰"""
        return {
            "answer": "æˆ‘æ˜¯AIè‡ªåª’ä½“å­¦ä¹ åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n1. åˆ¶å®š21å¤©å­¦ä¹ è®¡åˆ’\n2. åˆ†æçˆ†æ¬¾æ¡ˆä¾‹\n3. æä¾›è¿è¥å»ºè®®\n\nè¯·é—®æ‚¨æƒ³äº†è§£å“ªæ–¹é¢ï¼Ÿ",
            "type": "fallback",
            "suggestions": ["æŸ¥çœ‹21å¤©è®¡åˆ’", "åˆ†æçˆ†æ¬¾æ¡ˆä¾‹", "äº†è§£å˜ç°æ–¹å¼"]
        }
    
    def _generate_personalized_path(self, profile):
        """ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„"""
        # æ„å»ºç”¨æˆ·ç”»åƒæè¿°
        profile_desc = f"""
ç”¨æˆ·ç”»åƒï¼š
- æ—¶é—´æŠ•å…¥ï¼š{profile.get('timeAvailable', 'æœªçŸ¥')}
- AIæ°´å¹³ï¼š{profile.get('aiLevel', 'æœªçŸ¥')}
- å†…å®¹èƒ½åŠ›ï¼š{profile.get('contentSkill', 'æœªçŸ¥')}
- ç›®æ ‡ï¼š{profile.get('goal', 'æœªçŸ¥')}
- åå¥½å½¢å¼ï¼š{profile.get('format', 'æœªçŸ¥')}
- æ–¹å‘ï¼š{profile.get('direction', [])}
- ç»éªŒï¼š{profile.get('experience', [])}
"""
        
        # ä½¿ç”¨AIç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
        prompt = f"""åŸºäºä»¥ä¸‹ç”¨æˆ·ç”»åƒï¼Œç”Ÿæˆä¸€ä¸ª21å¤©çš„AIè‡ªåª’ä½“å­¦ä¹ è®¡åˆ’ï¼š

{profile_desc}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆ3ä¸ªå­¦ä¹ é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å«å…·ä½“ä»»åŠ¡ï¼š
1. é˜¶æ®µåç§°ã€æ—¶é•¿ã€é‡ç‚¹
2. 5ä¸ªå…·ä½“çš„å­¦ä¹ ä»»åŠ¡
3. ç»™å‡ºä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®

è¦æ±‚ï¼š
- æ ¹æ®ç”¨æˆ·çš„æ—¶é—´ã€èƒ½åŠ›å’Œç›®æ ‡å®šåˆ¶
- ä»»åŠ¡è¦å…·ä½“å¯æ‰§è¡Œ
- å¾ªåºæ¸è¿›ï¼Œç”±æµ…å…¥æ·±"""

        ai_response = call_gemini(prompt)
        
        if ai_response:
            return {
                "type": "ai_generated",
                "personalizedAdvice": ai_response,
                "profile": profile,
                "message": "å·²ä¸ºæ‚¨ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„"
            }
        else:
            # é™çº§å¤„ç†ï¼šè¿”å›åŸºç¡€è·¯å¾„
            return {
                "type": "default",
                "message": "åŸºäºæ‚¨çš„æƒ…å†µï¼Œæ¨èä»¥ä¸‹å­¦ä¹ è·¯å¾„",
                "path": self._get_default_path_for_profile(profile)
            }
    
    def _get_default_path_for_profile(self, profile):
        """æ ¹æ®ç”¨æˆ·ç”»åƒè¿”å›é»˜è®¤è·¯å¾„"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä¸åŒçš„ç”¨æˆ·ç±»å‹è¿”å›ä¸åŒçš„é¢„è®¾è·¯å¾„
        if profile.get('aiLevel', 0) <= 2:
            # æ–°æ‰‹è·¯å¾„
            return {
                "name": "AIæ–°æ‰‹è¿›é˜¶è·¯å¾„",
                "phases": [
                    {
                        "name": "åŸºç¡€è®¤çŸ¥",
                        "duration": "1-7å¤©",
                        "tasks": ["äº†è§£AIå·¥å…·", "å­¦ä¹ åŸºç¡€æ“ä½œ", "ç¡®å®šæ–¹å‘"]
                    },
                    {
                        "name": "å®è·µç»ƒä¹ ",
                        "duration": "8-14å¤©",
                        "tasks": ["åˆ›ä½œé¦–æ‰¹å†…å®¹", "ä¼˜åŒ–è¿­ä»£", "å»ºç«‹èŠ‚å¥"]
                    },
                    {
                        "name": "æå‡ä¼˜åŒ–",
                        "duration": "15-21å¤©",
                        "tasks": ["æ•°æ®åˆ†æ", "çˆ†æ¬¾ç ”ç©¶", "å˜ç°æ¢ç´¢"]
                    }
                ]
            }
        else:
            # è¿›é˜¶è·¯å¾„
            return {
                "name": "AIè¿›é˜¶ä¼˜åŒ–è·¯å¾„",
                "phases": [
                    {
                        "name": "å¿«é€Ÿå®šä½",
                        "duration": "1-4å¤©",
                        "tasks": ["èµ›é“åˆ†æ", "ç«å“ç ”ç©¶", "å·®å¼‚åŒ–å®šä½"]
                    },
                    {
                        "name": "å†…å®¹çˆ†å‘",
                        "duration": "5-14å¤©",
                        "tasks": ["æ‰¹é‡åˆ›ä½œ", "å¤šå¹³å°åˆ†å‘", "æ•°æ®ä¼˜åŒ–"]
                    },
                    {
                        "name": "å•†ä¸šå˜ç°",
                        "duration": "15-21å¤©",
                        "tasks": ["å¼€é€šæ”¶ç›Š", "å•†åŠ¡åˆä½œ", "ç§åŸŸè¿è¥"]
                    }
                ]
            }

def run_server(port=8000):
    """å¯åŠ¨æœåŠ¡å™¨"""
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸš€ AIè‡ªåª’ä½“å­¦ä¹ Agent - AIå¢å¼ºç‰ˆ          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ¨ å·²é›†æˆ Gemini-2.0-flash æ¨¡å‹
ğŸ”‘ ä½¿ç”¨ OpenRouter API

âœ… æœåŠ¡å¯åŠ¨æˆåŠŸï¼

ğŸ“ è®¿é—®åœ°å€ï¼š
   - APIæµ‹è¯•: http://localhost:{port}/
   - AIèŠå¤©: POST http://localhost:{port}/chat
   - å†…å®¹åˆ†æ: POST http://localhost:{port}/analyze
   - å­¦ä¹ è¿›åº¦: GET http://localhost:{port}/progress
   - çˆ†æ¬¾æ¡ˆä¾‹: GET http://localhost:{port}/cases

ğŸ¨ æ‰“å¼€ demo.html æŸ¥çœ‹Webç•Œé¢

æŒ‰ Ctrl+C åœæ­¢æœåŠ¡
""")
    
    server = HTTPServer(('localhost', port), AIHandler)
    server.serve_forever()

if __name__ == '__main__':
    try:
        run_server()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")